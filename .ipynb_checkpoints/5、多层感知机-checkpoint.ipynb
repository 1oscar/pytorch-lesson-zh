{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Variable变量 和 autograd机制\n",
    "\n",
    "先做一个热身题目，我们使用Tensor构建一个两层神经网络\n",
    "\n",
    "Tips:通常构建一个神经网络，我们有如下步骤\n",
    "\n",
    "1、构建好网络模型\n",
    "\n",
    "2、参数初始化\n",
    "\n",
    "3、前向传播\n",
    "\n",
    "4、计算损失\n",
    "\n",
    "5、反向传播求出梯度\n",
    "\n",
    "6、更新权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们构建神经网络之前，我们先介绍一个Tensor的内置函数 clamp()\n",
    "\n",
    "该函数的功能是：将输入 Tensor 的每个元素夹紧到区间 [min,max]中，并返回结果到一个新的Tensor。\n",
    "\n",
    "这样，我们就可以使用 x.clamp(min=0) 来代替 relu函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11128.106624737247\n",
      "521.4990869555062\n",
      "41.98206447797307\n",
      "4.215293391161581\n",
      "0.474883896909029\n",
      "0.057519924200060135\n",
      "0.00751843176731104\n",
      "0.001248899067986975\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# M是样本数量，input_size是输入层大小\n",
    "# hidden_size是隐含层大小，output_size是输出层大小\n",
    "M, input_size, hidden_size, output_size = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机数当作样本\n",
    "x = torch.randn(input_size, M) #size(1000, 64)\n",
    "y = torch.randn(output_size, M) #size(10, 64)\n",
    "\n",
    "# 参数初始化\n",
    "def init_parameters():\n",
    "    w1 = torch.randn(hidden_size, input_size)\n",
    "    w2 = torch.randn(output_size, hidden_size)\n",
    "    b1 = torch.randn(hidden_size, 1)\n",
    "    b2 = torch.randn(output_size, 1)\n",
    "    return {\"w1\": w1, \"w2\":w2, \"b1\": b1, \"b2\": b2}\n",
    "\n",
    "# 定义模型\n",
    "def model(x, parameters):\n",
    "    Z1 = parameters[\"w1\"].mm(x) #+ parameters[\"b1\"] # 线性层\n",
    "    A1 = Z1.clamp(min=0) # relu激活函数\n",
    "    Z2 = parameters[\"w2\"].mm(A1) #+ parameters[\"b2\"] #线性层\n",
    "    # 为了方便反向求导，我们会把当前求得的结果保存在一个cache中\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1}\n",
    "    return Z2, cache\n",
    "\n",
    "# 计算损失\n",
    "def loss_fn(y, y_pred):\n",
    "    m = y.size()[1] # m个样本\n",
    "    loss = (y_pred - y).pow(2).sum() # 我们这里直接使用 MSE(均方误差) 作为损失函数\n",
    "    return loss\n",
    "\n",
    "# 反向传播，求出梯度\n",
    "def backpropogation(x, y, y_pred, cache, parameters):\n",
    "    m = y.size()[1] # m个样本\n",
    "    # 以下是反向求导的过程：\n",
    "    d_y_pred = 1/m * (y_pred - y)\n",
    "    d_w2 = 1/m * d_y_pred.mm(cache[\"A1\"].t())\n",
    "    d_b2 = 1/m * torch.sum(d_y_pred, 1, keepdim=True)\n",
    "    d_A1 = parameters[\"w2\"].t().mm(d_y_pred)\n",
    "    # 对 relu 函数求导: start\n",
    "    d_Z1 = d_A1.clone()\n",
    "    d_Z1[cache[\"Z1\"] < 0] = 0\n",
    "    # 对 relu 函数求导: end\n",
    "    d_w1 = 1/m * d_Z1.mm(x.t())\n",
    "    d_b1 = 1/m * torch.sum(d_Z1, 1, keepdim=True)\n",
    "    grads = {\n",
    "        \"d_w1\": d_w1, \n",
    "        \"d_b1\": d_b1, \n",
    "        \"d_w2\": d_w2, \n",
    "        \"d_b2\": d_b2\n",
    "    }\n",
    "    return grads\n",
    "\n",
    "# 更新参数\n",
    "def update(lr, parameters, grads):\n",
    "    parameters[\"w1\"] -= lr * grads[\"d_w1\"]\n",
    "    parameters[\"w2\"] -= lr * grads[\"d_w2\"]\n",
    "    parameters[\"b1\"] -= lr * grads[\"d_b1\"]\n",
    "    parameters[\"b2\"] -= lr * grads[\"d_b2\"]\n",
    "    return parameters\n",
    "\n",
    "## 设置超参数 ##\n",
    "\n",
    "learning_rate = 1e-2\n",
    "EPOCH = 400\n",
    "\n",
    "# 参数初始化\n",
    "parameters = init_parameters()\n",
    "\n",
    "## 开始训练 ##\n",
    "for t in range(EPOCH):    \n",
    "    # 向前传播\n",
    "    y_pred, cache = model(x, parameters)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(loss)\n",
    "    # 反向传播\n",
    "    grads = backpropogation(x, y, y_pred, cache, parameters)\n",
    "    # 更新参数\n",
    "    parameters = update(learning_rate, parameters, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面的简单模型中，梯度计算的代码比较直观。但对于复杂的模型，例如多达数十层的神经网络，手动计算梯度非常困难。\n",
    "\n",
    "所以，我们需要引入自动求导 (autograd) 的机制，让pytorch在我们建立模型的时候，自动地为我们求出导数来。\n",
    "\n",
    "如何使用自动求导呢？\n",
    "\n",
    "**我们用一个 Variable $X$ 对象来包装Tensor变量。然后 $X$.data 就是该Tensor变量，$X$.grad 就是梯度咯**\n",
    "\n",
    "PyTorch的 Variable 跟 Tensor 有着几乎一样的API，在 Tensor 上进行的操作也基本都能在 Variable 上进行。**区别在于：使用 Variable 对象，你会定义一张动态图，它可以帮助你进行自动求导**\n",
    "\n",
    "现在，我们来使用 Variable 重新构建上述的两层神经网络，这个时候，我们已经不需要再使用手动求导了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11162.4453125\n",
      "313.19140625\n",
      "14.99639892578125\n",
      "0.9318702816963196\n",
      "0.06686659902334213\n",
      "0.005382555071264505\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable # 导入 Variable 对象\n",
    "\n",
    "# M是样本数量，input_size是输入层大小\n",
    "# hidden_size是隐含层大小，output_size是输出层大小\n",
    "M, input_size, hidden_size, output_size = 64, 1000, 100, 10\n",
    "\n",
    "# 生成随机数当作样本，同时用Variable 来包装这些数据，设置 requires_grad=False 表示在方向传播的时候，我们不需要求这几个 Variable 的导数\n",
    "x = Variable(torch.randn(input_size, M), requires_grad=False)\n",
    "y = Variable(torch.randn(output_size, M), requires_grad=False)\n",
    "\n",
    "# 参数初始化，同时用Variable 来包装这些数据，设置 requires_grad=True 表示在方向传播的时候，我们需要自动求这几个 Variable 的导数\n",
    "def init_parameters():\n",
    "    w1 = Variable(torch.randn(hidden_size, input_size), requires_grad=True)\n",
    "    w2 = Variable(torch.randn(output_size, hidden_size), requires_grad=True)\n",
    "    b1 = Variable(torch.randn(hidden_size, 1), requires_grad=True)\n",
    "    b2 = Variable(torch.randn(output_size, 1), requires_grad=True)\n",
    "    return {\"w1\": w1, \"w2\":w2, \"b1\": b1, \"b2\": b2}\n",
    "\n",
    "# 向前传播\n",
    "def model(x, parameters):\n",
    "    Z1 = parameters[\"w1\"].mm(x) + parameters[\"b1\"] # 线性层\n",
    "    A1 = Z1.clamp(min=0) # relu激活函数\n",
    "    Z2 = parameters[\"w2\"].mm(A1) + parameters[\"b2\"] #线性层\n",
    "    return Z2\n",
    "\n",
    "# 计算损失\n",
    "def loss_fn(y, y_pred):\n",
    "    m = y.size()[1] # m个样本\n",
    "    loss = (y_pred - y).pow(2).sum() # 我们这里直接使用 MSE(均方误差) 作为损失函数\n",
    "    return loss\n",
    "\n",
    "## 设置超参数 ##\n",
    "learning_rate = 1e-6\n",
    "EPOCH = 300\n",
    "\n",
    "# 参数初始化\n",
    "parameters = init_parameters()\n",
    "\n",
    "## 开始训练 ##\n",
    "for t in range(EPOCH):    \n",
    "    # 向前传播\n",
    "    y_pred= model(x, parameters)\n",
    "    # 计算损失\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    # 计算和打印都是在 Variables 上进行操作的，这时候的 loss 时一个 Variable ，它的size() 是 (1,)，所以 loss.data 的size() 也是 (1,)\n",
    "    # 所以， loss.data[0] 才是一个实值\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(loss.data[0])\n",
    "    # 使用自动求导来计算反向传播过程中的梯度，这个方法会把所有的设置了requires_grad=True 的Variable 对象的梯度全部自动出来\n",
    "    # 在这里，就是求出了 w1, w2, b1, b2的梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数， .data 表示的都是Tensor\n",
    "    parameters[\"w1\"].data -= learning_rate * parameters[\"w1\"].grad.data\n",
    "    parameters[\"w2\"].data -= learning_rate * parameters[\"w2\"].grad.data\n",
    "    parameters[\"b1\"].data -= learning_rate * parameters[\"b1\"].grad.data\n",
    "    parameters[\"b2\"].data -= learning_rate * parameters[\"b2\"].grad.data\n",
    "    \n",
    "    # 由于PyTorch中的梯度是会累加的，所以如果你没有手动清空梯度，那么下次你家的grad就是这次和上次的grad的累加和。\n",
    "    # 所以，为了每次都只是使用当前的梯度，我们需要手动清空梯度\n",
    "    parameters[\"w1\"].grad.data.zero_()\n",
    "    parameters[\"w2\"].grad.data.zero_()\n",
    "    parameters[\"b1\"].grad.data.zero_()\n",
    "    parameters[\"b2\"].grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
