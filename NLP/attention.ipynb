{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用Attention机制来加强我们的翻译效果\n",
    "\n",
    "上次的训练结果，大家也看到了 [使用Encoder-Decoder来完成机器翻译](https://github.com/LianHaiMiao/pytorch-lesson-zh/blob/master/NLP/encode_decoder.ipynb) ，很不幸，我们训练出了一个智障，那么有没有什么办法，可以提高它的“智商”呢？让它的翻译效果稍微提升一丢丢呢？\n",
    "\n",
    "有的！ 这就是 **attention 机制**。\n",
    "\n",
    "在上一篇中，我们的 decoder 在各个时刻使用了相同的背景向量。但是，如果解码器可以在不同时刻使用不同的背景向量呢，效果会不会更好呢？\n",
    "\n",
    "以 英语-中文 为例子，给定一个输入序列 \"I Love You\" 和输出序列 \"我爱你\" ，解码器在 t1 时刻可以使用更多编码了 “I” 的信息去解码生成 \"我\" ，在 t2 时刻可以使用更多编码了 \"Love\" 的信息去解码生成 \"爱\"。这听起来就像是解码器在不同的时刻对输入的数据有着不同的 “注意力” 这也就是注意力机制 (attention) 的由来。\n",
    "\n",
    "此时，相比于前一章节的模型，我们只需要更改 Decoder 部分的代码。\n",
    "\n",
    "此时 Decoder 模型的示意图是：\n",
    "\n",
    "\n",
    "![Decoder with attention](./images/attention-decoder-network.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
