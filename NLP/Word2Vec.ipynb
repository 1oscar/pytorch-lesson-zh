{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2Vec \n",
    "\n",
    "## 主要思想：\n",
    "\n",
    "**一个词的含义应该是由该词的上下文来决定的。**\n",
    "\n",
    "1、我们有一个很大的语料库，这个语料库包含了很多的 text\n",
    "\n",
    "2、所有的单词都可以用一个向量表示\n",
    "\n",
    "3、我们使用一个中心词 $c$ 和上下文 $o$ 来遍历 text 中的每个位置 $t$\n",
    "\n",
    "4、使用 $c$ 和 $o$ 的词向量的相似度来计算 $P(o|c)$\n",
    "\n",
    "5、最大化这个概率。\n",
    "\n",
    "6、上述步骤结束之后，我们得到的权重矩阵，就可以看作是单词的 vector\n",
    "\n",
    "\n",
    "### 两个模型：\n",
    "\n",
    "* skip-grams: 给定单词来预测上下文\n",
    "\n",
    "* CBOW: 给定上下文来预测单词\n",
    "\n",
    "### 两个高效的训练方法：\n",
    "\n",
    "* Hierarchical Softmax\n",
    "\n",
    "* Negative sampling\n",
    "\n",
    "\n",
    "这里的模型 + 训练方法一共可以有四种组合。\n",
    "\n",
    "这里，只会写 skip-grams 模型以及 skip-grams + Negative sampling，其余内容如果有兴趣，可以自行编写，或者以后我想写了再补齐（就是这么任性~）\n",
    "\n",
    "这里只简单地讲述主要思想和实现方式，更多的具体内容，可以观看 [CS224N 第二课](https://www.bilibili.com/video/av13383754/?from=search&seid=17986192997424429774#page=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## skip-grams模型\n",
    "\n",
    "\n",
    "奉献一张，我心中认为的解读 skip-grams模型 最棒的图，一图胜千言。\n",
    "\n",
    "![skip-grams](./word2vec/skipgram.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
