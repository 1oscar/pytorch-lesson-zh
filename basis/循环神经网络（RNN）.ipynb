{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (循环神经网络)\n",
    "\n",
    "以前我们写过的 Network 的结果都是反馈给下一层。\n",
    "\n",
    "那在RNN中，我们不仅要输出结果，还要输出一个**隐含状态**，给当前层在处理下一个样本时使用。\n",
    "\n",
    "结构图如下所示：\n",
    "\n",
    "![Recurrent Neural Networks have loops](../images/RNN/RNN-rolled.jpg)\n",
    "\n",
    "\n",
    "可以看到，跟普通的 Neural Network 相比，我们这里仅仅是多了一个**隐含状态**，那么这个多出来的**隐含状态**有什么好处呢？\n",
    "\n",
    "这个**隐含状态**，我们可以看作是上一个样本遗留给我们的信息，通过这个信息，我们可以把握过去样本的特征，结合当前的样本，组成一种类似于 “时间序列” 样本信息，从而让模型有了处理前后有依赖关系数据样本的能力。\n",
    "\n",
    "**举个例子:**\n",
    "\n",
    "语言模型的任务是给定句子的前 $t$ 个字符，然后预测第 $t+1$ 个字符。假设我们的句子是“你好世界”，使用循环神经网络来预测的一个做法是，在时间 $1$ 输入“你”，预测“好”同时生成一个**隐含状态**；\n",
    "\n",
    "在时间 $2$ 向同一个网络输入刚才生成的**隐含状态** 和 “好” 去预测“世”。同时输出一个更新过的隐含状态。\n",
    "\n",
    "对于这个**隐含状态**， 我们会希望前面的信息能够保存在这个隐含状态里，从而提升预测效果。下图展示了这个过程。\n",
    "\n",
    "![Recurrent Neural Networks have loops](../images/RNN/RNN.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RNN的公式表示\n",
    "\n",
    "针对输入输出加上时间状态 $t$。在 $t$ 时刻，我们的输入为 $X_t$ 和 $H_{t-1}$，经过一个隐含层之后，我们有输出 $H_t$，经过最后的输出层得到 $\\hat{Y_t}$。\n",
    "\n",
    "其中 $X_t \\in R^{n \\times x}$ (样本数量为 $n$，每个样本特征向量维度是 $x$) ； $H_{t-1} \\in R^{n \\times h}$ （其中隐含层长度为 $h$）\n",
    "\n",
    "隐含层的计算公式为：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "其中 $\\mathbf{W}_{xh}$ 和 $\\mathbf{W}_{hh}$ 是隐含层的权重， $\\mathbf{b}_h$ 是隐含层的 bias。\n",
    "\n",
    "输出层的计算公式为：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "其中 $\\mathbf{W}_{hy}$ 是输出层权重，$\\mathbf{b}_y$ 是输出层的 bias\n",
    "\n",
    "\n",
    "最开始的隐含状态里的元素通常会被初始化为0，也就是 $H_0$ 是个零矩阵。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "循环神经网络的简单介绍就到这里了，如果想要更加细致的了解，可以看本人写过的一篇Blog [详细剖析RNN理论、变种和应用](https://lianhaimiao.github.io/2018/02/09/%E8%AF%A6%E7%BB%86%E5%89%96%E6%9E%90RNN%E7%90%86%E8%AE%BA%E3%80%81%E5%8F%98%E7%A7%8D%E5%92%8C%E5%BA%94%E7%94%A8/#more)\n",
    "\n",
    "\n",
    "理论部分我们已经了解了，那么下实际操作中，我们如何用 PyTorch 写一个RNN呢？\n",
    "\n",
    "\n",
    "答案很简单，就是一个函数...\n",
    "\n",
    "```\n",
    "torch.nn.RNN()\n",
    "```\n",
    "\n",
    "对的，你没看错，实现 RNN 就是这么简单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
