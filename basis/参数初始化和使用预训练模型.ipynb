{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数初始化\n",
    "\n",
    "**本章内容作为补充内容，可以后期有需要使用别人已经预训练好了的模型再回来翻**\n",
    "\n",
    "\n",
    "对于PyTorch中的各种网络层，我们会需要对参数进行初始化的操作，这里我们以 nn.Linear() 为例子：\n",
    "\n",
    "```\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "    ...\n",
    "        self.reset_parameters()\n",
    "    ...\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "```\n",
    "\n",
    "上述代码摘自 PyTorch [torch.nn.Linear()的源码](http://pytorch.org/docs/master/_modules/torch/nn/modules/linear.html#Linear)\n",
    "\n",
    "从源代码中可以看到，PyTorch在我们实例化某层的时候，就帮我们自动完成了初始化的操作。但是，有时候，我们可能需要一些不一样的初始化。\n",
    "\n",
    "比如使用 xavier 方法 来进行初始化，我们该怎么做呢？\n",
    "\n",
    "在PyTorch中有一个 torch.nn.init 这个包就是用来帮我们完成初始化的。\n",
    "\n",
    "这个包提供了很多初始化的方法：\n",
    "\n",
    "* torch.nn.init.constant(tensor, val)\n",
    "* torch.nn.init.normal(tensor, mean=0, std=1)\n",
    "* torch.nn.init.xavier_uniform(tensor, gain=1)\n",
    "\n",
    "详情 http://pytorch.org/docs/master/nn.html#torch-nn-init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, n_class)\n",
    "        \n",
    "        # 参数初始化函数\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Conv2d):\n",
    "                nn.init.xavier_normal(p.weight.data)\n",
    "            elif isinstance(p, nn.Linear):\n",
    "                nn.init.normal(p.weight.data)\n",
    "\n",
    "    # 向前传播\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用预训练的AlexNet模型来对CIFAR10进行训练\n",
    "\n",
    "现在，假设，我们知道AlexNet是个好用的模型，我们想用它来对CIFAR10进行训练，但是我们不想自己训练，想用别人训练好的，我们该怎么做呢？\n",
    "\n",
    "分三步：\n",
    "\n",
    "1、加载预训练模型\n",
    "\n",
    "2、对模型结构进行更改\n",
    "\n",
    "3、重新训练，我们需要进行训练的那几层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------------get Alexnet model-------------------------\n",
    "def getAlexNet(DOWNLOAD=True):\n",
    "    alexnet = models.alexnet(pretrained=DOWNLOAD)\n",
    "    return alexnet\n",
    "\n",
    "# -----------------revise the AlexNet class--------------------------\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def to_var(x):\n",
    "    x = Variable(x)\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "DOWNLOAD = True\n",
    "# 下载预训练好的AlexNet模型\n",
    "pre_alexnet = getAlexNet(DOWNLOAD)\n",
    "pretrain_dict = pre_alexnet.state_dict()\n",
    "\n",
    "# 因为 CIFAR10 只有10个种类，我们设置 num_classes=10\n",
    "alexnet = AlexNet(10)\n",
    "alexnet_dict = alexnet.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrain_dict.items() if k in alexnet_dict}\n",
    "\n",
    "# 更新我们自己设置的Alexnet网络的权重\n",
    "alexnet_dict.update(pretrained_dict)\n",
    "# 将这些权重加载到模型中。\n",
    "alexnet.load_state_dict(alexnet_dict)\n",
    "\n",
    "# 使用GPU\n",
    "if torch.cuda.is_available():\n",
    "    alexnet = alexnet.cuda()\n",
    "\n",
    "# get Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Scale(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.CIFAR10('./data', train=False, transform=transform, download=False)\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=256, shuffle=True)\n",
    "\n",
    "# optim\n",
    "learning_rate = 0.0001\n",
    "num_epoches = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 训练的时候，我们只更新 fc 层的参数\n",
    "optimizer = optim.Adam(alexnet.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# training\n",
    "alexnet.train()\n",
    "for epoch in range(num_epoches):\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    runnin_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    for data, label in train_data_loader:\n",
    "        img = to_var(data)\n",
    "        label = to_var(label)\n",
    "        out = alexnet(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data[0] * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        accuracy = (pred == label).float().mean()\n",
    "        runnin_acc += num_correct.data[0]\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Acc: {:.6f}\".format(runnin_acc / len(train_data)))\n",
    "\n",
    "# evaluation\n",
    "alexnet.eval()\n",
    "runnin_acc = 0.0\n",
    "for data, label in test_data_loader:\n",
    "    img = to_var(data)\n",
    "    label = to_var(label)\n",
    "    out = alexnet(img)\n",
    "    loss = criterion(out, label)\n",
    "    _, pred = torch.max(out, 1)\n",
    "    num_correct = (pred == label).sum()\n",
    "    accuracy = (pred == label).float().mean()\n",
    "    runnin_acc += num_correct.data[0]\n",
    "print(\"Acc: {:.6f}\".format(runnin_acc / len(test_data)))\n",
    "\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
