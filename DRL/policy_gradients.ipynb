{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务说明\n",
    "\n",
    "gym的Cart Pole环境. Cart Pole 在OpenAI的gym模拟器里面，是相对比较简单的一个游戏。游戏里面有一个小车，上有竖着一根杆子。小车需要左右移动来保持杆子竖直。如果杆子倾斜的角度大于15°，那么游戏结束。小车也不能移动出一个范围（中间到两边各2.4个单位长度）。如下图所示： \n",
    "\n",
    "![cart_pole](./images/cart_pole.gif)\n",
    "\n",
    "在gym的Cart Pole环境（env）里面，左移或者右移小车的action之后，env都会返回一个+1的reward。到达200个reward之后，游戏也会结束。\n",
    "\n",
    "\n",
    "# 任务要求\n",
    "\n",
    "需要让电脑学会玩这个简单的小游戏.\n",
    "\n",
    "\n",
    "# 预备知识\n",
    "\n",
    "## 1. gym 的安装\n",
    "\n",
    "请参考 gym 在 github 上的仓库   [gym链接](https://github.com/openai/gym)\n",
    "\n",
    "\n",
    "## 2. 强化学习的基本知识\n",
    "\n",
    "强化学习是一种算法, 是让计算机自己慢慢摸索, 通过不断地尝试, 从错误中学习, 最后找到规律, 学会了达到目的的方法. 按照强化学习一贯的说法,我们会将整个学习过程看成一个三元组 $(o, a, r)$ 和 环境 env , 其中 $o$ 表示观察, $a$ 表示行动, $r$ 表示奖励, env 表示环境. 在强化学习的过程中, 我们根据环境得到当前环境的表示(也就是 $o$ 观察), 接着,根据当前的观察 $o$ 我们选择一个合适的 行动 $a$, 并且这个行动 $a$ 可以让我们得到较高的奖励 $r$. \n",
    "以 Alpha Go 为例. 在 alpha Go 的学习过程中, 旗手每下一次旗,计算机都会得到一个观察 $o$, 针对这个观察 $o$, 计算机会选择一个奖励较高的 行动 $a$ 去进行落子的行为. 于是,这样一个强化学习的过程就产生了. \n",
    "\n",
    "再通俗点说, 就是在强化学习的过程中,会有一个\"监督\"的过程(这里的监督必须打引号!!). 这个监督的过程指的是计算机在学习的过程中,会有一个虚拟的老师,这个虚拟老师只会做一件事,那就是给我们的每个行动进行打分.并且学习的最终目的是**让计算机得到的分数尽可能的高**.所以,我们在学习的过程中会尽量只记住那么得到高分的行动,避免得到低分的行动.\n",
    "\n",
    "比如老师会根据我的开心程度来打分, 我开心时, 可以得到高分, 我不开心时得到低分. 有了这些被打分的经验, 我就能判断为了拿到高分, 我应该选择一张开心的脸, 避免选到伤心的脸. 这也是强化学习的核心思想. 可以看出在强化学习中, 一种行为的分数是十分重要的. 所以强化学习具有分数导向性. 我们换一个角度来思考.这种分数导向性好比我们在监督学习中的正确标签.\n",
    "\n",
    "我们知道监督学习, 是已经有了数据和数据对应的正确标签, 比如这样. 监督学习就能学习出那些脸对应哪种标签. 不过强化学习还要更进一步, 一开始它并没有数据和标签.\n",
    "\n",
    "他要通过一次次在环境中的尝试, 获取这些数据和标签, 然后再学习通过哪些数据能够对应哪些标签, 通过学习到的这些规律, 竟可能地选择带来高分的行为 (比如这里的开心脸). 这也就证明了在强化学习中, 分数标签就是他的老师, 他和监督学习中的老师也差不多.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Policy Gradient 算法\n",
    "\n",
    "推荐观看李宏毅老师的课程, [B站链接](https://www.bilibili.com/video/av24724071/?p=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习的整体框架\n",
    "\n",
    "1. 定义环境\n",
    "\n",
    "2. 选择行为\n",
    "\n",
    "3. 计算奖励\n",
    "\n",
    "4. 梯度上升(因为 Policy Gradient 中是求最大的奖励) PS:最大奖励可以看成是最小奖励求负数\n",
    "\n",
    "\n",
    "> 上述四个过程, gym 已经帮我们解决了两个, 分别是 定义环境 和 计算奖励, 所以,我们接下来需要做的就是选择行为 和 梯度上升.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
